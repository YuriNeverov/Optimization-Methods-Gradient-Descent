# Optimization-Methods-Gradient-Descent

## Задание
1. Реализуйте градиентный спуск с постоянным шагом (learning rate). Исследовать сходимость при различных значениях шага.
2. Подберите функцию изменения шага (learning rate scheduling), чтобы улучшить сходимость. Например экспоненциальную или ступенчатую.
3. Реализовать какой-нибудь метод одномерного поиска. (дихотомия, метод Фибоначчи, метод золотого сечения) и градиентный спуск на его основе. Сравнить эффективность с точки зрения количества вычислений минимизируемой функции и ее градиента.
4. *Сделать одномерный поиск с учетом условий Вольфе и исследовать эффективность.
5. Проанализируйте траекторию градиентного спуска для нескольких квадратичных функций: придумайте две-три двумерные квадратичные функции, на которых работа метода будет отличаться, нарисуйте график с линиями уровня функций и траекториями методов.   
   Попробуйте ответить на следующий вопрос: Как отличается поведение метода в зависимости от числа обусловленности функции, выбора начальной точки и стратегии выбора шага?
6. Исследуйте как зависит число итераций, необходимое градиентному спуску для сходимости, от следующих двух параметров:
   1. число обусловленности `k ≤ 1` оптимизируемой функции
   2. размерности пространства `n` оптимизируемых переменных.
   
   Для этого для заданных параметров `n` и `k` сгенерируйте случайным образом квадратичную задачу размера `n` с числом обусловленности `k` и запустите в ней градиентный спуск с некоторой фиксируемой требуемой точностью. Замерьте число итераций `T(n, k)`, которое потребовалось сделать методу до сходимости (успешному выходу по критерию остановки). 


